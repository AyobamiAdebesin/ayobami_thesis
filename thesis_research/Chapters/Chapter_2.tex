\chapter{METHODOLOGY AND ALGORITHM DESCRIPTION}

In this chapter, we shall present a detailed description of the methodologies and implementation of algorithms used in this thesis to solve the generalized eigenvalue problem. We begin by describing the problem setup, followed by a discussion of the algorithms used, together with their implementation details. This chapter aims to provide a comprehensive understanding of how these algorithms are applied to derive the solutions to the problem at hand. We shall also give a description of the numerical experiments we setup to investigate the efficiency of these algorithms.

To compute the eigenvalues and eigenvectors that satisfy (\ref{eq:GenEigVal_Problem}) with spectral transformation lanczos algorithm, our approach will be in two steps:
\begin{itemize}
	\item[$\bullet$] Transform the generalized problem into a spectral transformed standard eigenvalue problem.
	\item[$\bullet$] Solve the spectral problem with Lanczos algorithm.
\end{itemize}
As described in Section~\ref{sec:SpectralTransformationDefinition}, 
Lemma~\ref{lemma:SpectralTransLemma} gives us the relationship between the eigenvalues (\textit{resp.}\ eigenvectors) of the original problem and the eigenvalues (\textit{resp.}\ eigenvectors) of the spectral transformed problem. We also recall that solving \ref{eq:SpectralTransEquation} relies on symmetric factorization of the $A - \sigma B$ and $B$. The following section describes the possible ways of doing this.
\section{Stable Decompositions}\label{sec:StableDecompositions}
We shall begin by computing decompositions for $A - \sigma B$ and $B$. If $B$ is positive  definite, we can compute a Cholesky decomposition $B = C_bC_b^T$ using the SciPy \texttt{cholesky} method which calls LAPACK \textbf{\texttt{xPOTRF}}. However, if $B$ is semi positive definite, this function call fails and we use the more robust pivoted Cholesky factorization \textbf{\texttt{xPSTRF}} by calling the inbuilt LAPACK bindings in SciPy.

There are various possible factorization options for $A-\sigma B$. One option is to use the pivoted $LDL^{T}$ factorization used by \cite{stewart2024spectraltransformationdensesymmetric} and \cite{Ericsson1980TheST} where $D$ is a block diagonal matrix with $1 \times 1$ and $2 \times 2$ on the diagonal, and $L$ is a lower triangular matrix. This factorization ideally uses the ``rook pivoting'' scheme, which is stable. Although the standard $LDL^T$ factorization (without ``rook pivoting'') is available in the SciPy linear algebra module, there is no option to use the rook pivoting scheme except if one chooses to write a custom LAPACK binding that makes use of \textbf{\texttt{DSYTRF\_ROOK}}. While this can guarantee some stability for the problem we are trying to solve, it usually involves extra work in processing the $2 \times 2$ blocks to make $D$ diagonal.

Another possible choice of factorization is an eigenvalue decomposition of $A - \sigma B$. If we use a symmetric eigenvalue decomposition $A- \sigma B = WDW^T$, our numerical experiments reveals that this stabilizes the Ritz residuals and generalized form of the residuals together with the advantage that these residuals are insensitive to ill-conditioning in $A-\sigma B$. This can be done using inbuilt eigenvalue solvers in SciPy or any linear algebra library. This is the most promising factorization, however computing eigenvalue decompositions for large sparse problems become computationally expensive and not feasible in reality. So, although the eigenvalue decomposition allows tests of stability, in a real implementation it would be replaced by an $LDL^T$ factorization.

Lastly, we could make use of an $LU$ factorization for $A-\sigma B$. Unlike the previous factorizations, the stability for the Ritz residuals is not as great, as we observe that they depend on the conditioning of $A-\sigma B$.  However, for the purpose of this thesis, we make use of the $LU$ decomposition since it is computationally less expensive and easy to use and implement. Comparing it to the eigenvalue decomposition of $A-\sigma B$ also illustrates a potential mechanism of instability in implementing the spectral shift.

One major takeaway from our experiments with the various options of factorizing $A-\sigma B$ is that symmetry is clearly important for stability. We plan to give a mathematical justification for this in future work.

Given our recent results, one might suggest using a stable decomposition such as the $LU$ factorization. However, as we will demonstrate, decompositions that preserve symmetry exhibit certain stability advantages in practice.

\section{The Lanczos decomposition}\label{sec:LanczosDecomposition}

In this section, we revisit the Lanczos algorithm, and discuss how we apply it to the spectral transformed problem. As discussed in section (\ref{sec:LanczosAlgorithm}), the Lanczos algorithm approximates the eigenvalues of the original problem by projecting it onto a Krylov subspace spanned by successive powers of the system matrix applied to an initial vector. The eigenvalue approximations arises from the tridiagonal matrix obtained through the Lanczos process, which captures the essential spectral characteristics of the original matrix.

Given $A \in \mathbb{R}^{m \times m}$, with $A=A^T$, the pesudocode for the lanczos algorithm is described by Algorithm~\ref{alg:lanczos_algorithm}.
\begin{algorithm}
	\caption{Lanczos Algorithm for a Symmetric Matrix}
	\label{alg:lanczos_algorithm}

	\textbf{Require:} \( A = A^T \), number of iterations: \(n\), tolerance: \(tol\)
	\begin{algorithmic}[1]
		\Function{lanczos}{$A, n, tol$}
		\State Choose an arbitrary vector $\mathbf{b}$ and set an initial vector $\mathbf{q_1} = \mathbf{b}/ \|\mathbf{b}\|_2$ 
		\State Set $\delta_0 = 0$ and $\mathbf{q_0} = \mathbf{0}$
			\For{$j = 1, 2, \dots, n$}
		\State $\mathbf{v} = A \mathbf{q_j}$
		\State $\gamma_j = \mathbf{q_j}^T \mathbf{v} $
		\State $\mathbf{v} = \mathbf{v} - \delta_{j-1}\mathbf{q_{j-1}} - \gamma_j \mathbf{q_j}$
		\State \textbf{Full reorthogonalization:} $\mathbf{v} = \mathbf{v} - \sum_{i \leq j} (\mathbf{q_i}^T \mathbf{v}) \mathbf{q_i}$
		\State $\delta_{j} = \|\mathbf{v}\|_2$
		\If{$\delta_{j} < tol $}
		\State \textbf{restart} or \textbf{exit}
		\EndIf
		\State $\mathbf{q_{j+1}} := \mathbf{v} / \delta_{j}$
		\EndFor
		\EndFunction
	\end{algorithmic}
\end{algorithm}
After the completion of Algorithm~\ref{alg:lanczos_algorithm}, the $\gamma_j$ and $\delta_j$ are used to construct the tridiagonal matrix $T_n \in \mathbb{R}^{n \times n}$ and the vectors $\mathbf{q}_j$ are stacked together to form an orthogonal matrix $Q_n \in \mathbb{R}^{m \times n}$ given by:
\[T_n = \begin{pmatrix}
			\gamma_1 & \delta_1 & & & \\\delta_1 & \gamma_2 & \delta_2 & & \\ & \delta_2 & \gamma_3 & \delta_3 & \\ & & \ddots & \ddots & \vdots \\ & & & \delta_{n-1} & \gamma_n
		\end{pmatrix}\] 
	\[
	Q_n = 
	\begin{bmatrix}
		 & \big| &  & \big| &  & \big| &  \\
		 & \big| &  & \big| &  & \big| &  \\
		 \mathbf{q_1} & \big| & \mathbf{q_2} & \big| & \cdots & \big| & \mathbf{q_n} \\
		 & \big| &  & \big| &  & \big| &  \\
		 & \big| &  & \big| &  & \big| &  \\
	\end{bmatrix}.
	\]
The decomposition is given by
\begin{equation}\label{eq:Lanczos_Decomposition}
	AQ_n = Q_nT_n + \delta_{n}\mathbf{q_{n+1}}\mathbf{e_n}^T
\end{equation}
In theory, the vectors $q_j$'s should be orthonormal, but due to floating-point errors, there will be loss of orthogonalization, hence the need for line $8$ in Algorithm~\ref{alg:lanczos_algorithm}.

Let $\theta_i, i = 1,2, \ldots n$ (which can be computed by standard functions using any eigenvalue solver) be the eigenvalues of $T_n$, and $\{\mathbf{y_i}\}_{i = 1 : n}$ be the associated eigenvectors. The $\{\theta_i\}$ are called the \textit{Ritz values} and the vectors $\{Q_n\mathbf{y_i}\}_{i = 1 : n}$ are called the \textit{Ritz vectors}. Hence, the eigenvalues of $A$ are on both ends of the are well approximated by the Ritz values, with the Ritz vectors as their approximate corresponding eigenvectors of $A$.

Since the generalized eigenvalue problem we started with has been reduced to a standard one as shown in equation (\ref{eq:SpectralTransEquation}), Algorithm~\ref{alg:lanczos_algorithm} can be applied to equation (\ref{eq:SpectralTransEquation}) with some slight modifications. The spectral form of Algorithm~\ref{alg:lanczos_algorithm} is given by Algorithm~\ref{alg:spectral_lanczos_algorithm}.
\begin{algorithm}
	\caption{Spectral Lanczos Algorithm for (\ref{eq:SpectralTransEquation}) }
	\label{alg:spectral_lanczos_algorithm}
	
	\textbf{Require:} \( A = A^T \), \( B = B^T \), with \(B\) being positive definite or semidefinite\\
	\textbf{Require:} number of iterations: \(n\), size of matrix $A$ or $B$: $m$, tolerance: \(tol\)\\
	\textbf{Require:} \(\sigma \in \mathbb{R}\): shift not close to a generalized eigenvalue
	\begin{algorithmic}[1]
		\Function{\textsc{Spectral\_Lanczos}}{$A, B, m, n, \sigma, tol$}
		\State Choose an arbitrary vector $\mathbf{b}$ and set an initial vector $\mathbf{q_1} = \mathbf{b}/ \|\mathbf{b}\|_2$
		\State Set $\beta_0 = 0$ and $\mathbf{q_0} = \mathbf{0}$
		\State Set $Q = \text{zeros}(m, n+1)$
		\State Precompute the $LU$ factorization of $A - \sigma B$: $LU = (A - \sigma B)$
		\State Factor: $B = CC^T$
		\For{$j = 1, 2, \dots, n$}
		\State $Q[:, j] = \mathbf{q}_j$
		\State $\mathbf{u} = C\mathbf{q}_j$
		\State Solve: $(LU)\mathbf{v} = \mathbf{u}$ for $\mathbf{v}$
		\State $\mathbf{v} = C^T \mathbf{v}$
		\If{$j < n $}
		\State $\alpha_j = \mathbf{q}_j^T \mathbf{v} $
		\State $\mathbf{v} = \mathbf{v} - \beta_{j-1}\mathbf{q}_{j-1} - \alpha_j \mathbf{q}_j$
		\State \textbf{Full reorthogonalization:} $\mathbf{v} = \mathbf{v} - \sum_{i \leq j} (\mathbf{q}_i^T \mathbf{v}) \mathbf{q}_i$
		\State $\beta_{j} = \|\mathbf{v}\|_2$
		\If{$\beta_{j} < tol $}
		\State \textbf{restart} or \textbf{exit}
		\EndIf
		\State $\mathbf{q}_{j+1} := \mathbf{v} / \beta_{j}$
		\EndIf
		\EndFor
		\State $Q = Q[:, :n]$
		\State $\mathbf{q} = Q[:, n]$
		\State \Return $(Q, T, \mathbf{q})$
		\EndFunction
	\end{algorithmic}
\end{algorithm}
After applying the lanczos procedure to the spectral transformed problem (\ref{eq:SpectralTransEquation}), we compute the converged Ritz pairs using a certain tolerance. The converged Ritz pairs are then mapped to the generalized eigenvalues and eigenvectors where we can observe the behaviour of these residuals with respect to conditioning.

\section{Problem Setup}\label{sec:ProblemSetup}

To evaluate the performance and robustness of the spectral transformation lanczos algorithm, we setup a problem with predetermined eigenvalues, use the algorithm to compute the eigenvalues, and show that the residuals follow closely with the bounds predicted for direct methods in \cite{stewart2024spectraltransformationdensesymmetric}. While there are other options of using matrices from open source repositories like Matrix Market, we choose to use this approach so that we can control the size, condition number and other properties of the matrix so as to observe the effect of this properties on the algorithm.

Starting with a diagonal matrix $D \in \mathbb{R}^{m \times m}$ with known eigenvalues, we generate a random matrix $P$ of size $m \times m $ with standard normal distribution. Since the $QR$ factorization is guaranteed to exist for any matrix, we take the $QR$ factorization of $P$ to obtain an orthogonal matrix $Q$, which is used to create a matrix $C$ using orthogonal transformation. Hence $C = QDQ^T$ is unitarily similar to $D$.

Next, we initialize a random lower triangular matrix $L_0 \in \mathbb{R}^{m \times m}$ with a normal distribution. A symmetric positive definite $B \in \mathbb{R}^{m \times m}$ is formed by

\begin{equation}\label{eq:FormingB}
	B = L_0 L_0^T + \delta I_m, \qquad \delta > 0,
\end{equation}
where $I_m$ is an identity matrix of order $m$. Clearly, $B$ is symmetric. The matrix $L_0L_0^T$ is positive semi-definite since for any non-zero vector $\mathbf{x}$
\begin{equation}
	\mathbf{x}^T(L_0L_0^T)\mathbf{x} = (L_0^T\mathbf{x})^T(L_0^T\mathbf{x}) = \| L_0^T\mathbf{x} \|^2 \geq \mathbf{0}.
\end{equation}
However, $L_0L_0^T$ may not be strictly positive definite if $L_0$ is singular. The term $\delta I_m$ ensures strict positve definiteness by adding $\delta$ to its diagonals, thereby shifting all eigenvalues by $\delta$. If $\delta > 0$, then all eigenvalues of $B$ will be strictly positive, ensuring $B$ is positive definite. This guarantees that we can compute the Cholesky factorization of $B$ without any numerical issues.

Another important thing to note is that, $\delta$ can be used to control the conditioning of $B$. We recall from section (\ref{sec:ConditioningAndStability}), that the condition number of $B$ when $B$ is symmetric, is defined as:
\begin{equation}
	\kappa(B) = \frac{\lambda_{\max}(B)}{\lambda_{\min}(B)}
\end{equation}
where $\lambda_{\max}(B)$ and $\lambda_{\min}(B)$ are the largest and smallest eigenvalues of B, respectively.
In general, $B$ is usually ill-conditioned with a very large condition number so that if $\delta$ is large, the process of adding $\delta I_m$ can regularize the condition number of $B$, making $B$ well-conditioned, since that will equate to increasing $\lambda_{\min}(B)$. If $\delta$ is small, $B$ can still be ill-conditioned but not in an astronomical way. Hence, $\delta$ is a hyperparameter we can use to control the condition of $B$. In this experiment, we choose $\delta = 10^{1}$, which gives a condition number of $\kappa(B) = 8.09 \times 10^2$.

Since $B$ is symmetric and positive definite, we can compute its Cholesky factorization $B = LL^T$ and construct $A$ using a congruence transformation
\begin{equation}\label{eq:FormingA}
	A = LCL^T
\end{equation}
So that the generalized eigenvalues $\Lambda(A, B)$ is equal to the eigenvalues of the diagonal matrix $D$. This can be summarized by the following lemma:
\begin{lemma}
  Let $A-\lambda B$ be a pencil, where $A$ and $B$ are symmetric, and $B$ is strictly positive definite. Let $D$ be a diagonal matrix and $C$ be unitarily similar to $D$. Assuming (\ref{eq:FormingA}) holds, then the  generalized eigenvalues \added{in} $\Lambda(A, B)$ \rep{are the diagonal elements of}{is similar to} $D$
\end{lemma}

\begin{proof}
	Given the generalized problem
	\begin{equation}
		A \mathbf{v} = \lambda B \mathbf{v}, \qquad \mathbf{v} \neq \mathbf{0}
	\end{equation}
	Since $B$ is positive definite, then clearly, it is invertible and the generalized eigenvalues $\Lambda(A, B)$ will be the eigenvalues of $B^{-1}A$.

	Now
	\begin{align*}
		B^{-1}A & = (LL^T)^{-1}(LCL^T)\\
		& = L^{-T}L^{-1}LQDQ^{T}L^T\\
		& = (L^{-T}Q)D(Q^{-1}L^{T}) \\
		& = (L^{-T}Q)D(L^{-T}Q)^{-1}
	\end{align*}
	Therefore $B^{-1}A$ is similar to $D$\del{and hence $\Lambda(A,B)$ is similar to $D$}.
\end{proof}
The pseudocode for generating $A$ and $B$ is described in Algorithm~\ref{alg:problem_setup}.
\begin{algorithm}
	\caption{Setting up a GEP}
	\label{alg:problem_setup}
	
	\textbf{Require:} \( D \): diagonal matrix with known eigenvalues, \(\delta\): regularization hyperparameter
	\begin{algorithmic}[1]
		\Function{\textsc{Generate\_Matrix}}{$D, \delta$}
		\State Set $m$ = \texttt{size}($D$)
		\State $Q$, \_\_ = \texttt{qr}(random.randn($m$, $m$))
		\State $C = QDQ^T$
		\State $L_{0}$ = \texttt{tril}(\texttt{random.randn}($m$, $m$))
		\State $B = (L_0 L_0^T) + \delta I$
		\State $L$ = \texttt{cholesky}($B$)
		\State $A$ = $LCL^T$
		\State \Return ($A$, $B$)
		\EndFunction
	\end{algorithmic}
\end{algorithm}
With the problem setup completed, and the algorithm described, in the next chapter, we shall discuss the results obtained in these experiments.


%%% Local Variables:
%%% mode: LaTeX
%%% TeX-master: "../main"
%%% End:
