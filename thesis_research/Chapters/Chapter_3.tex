\chapter{METHODOLOGY AND ALGORITHM DESCRIPTION}
\newtheorem{lemma}[theorem]{Lemma}
\section{Spectral Transformation}
In this chapter, we shall present a detailed description of the methodologies and implementation of algorithms used in this thesis to solve the generalized eigenvalue problem. We begin by describing the problem setup, followed by a discussion of the algorithms used, together with their implementation details. This chapter aims to provide a comprehensive understanding of how these algorithms are applied to derive the solutions to the problem at hand. We shall also give a description of the numerical experiments we setup to investigate the efficiency of these algorithms.\\
Consider the symmetric-definite generalized eigenvalue problem:
\begin{equation}\label{3.1}
	A\mathbf{v} = \lambda B\mathbf{v}, \qquad \mathbf{v} \neq 0
\end{equation}
where $A$ and $B$ are $m \times m$ real, sparse, symmetric and $B$ is positive definite or positive semi-definite.\\
Problem (\ref{3.1}) can be reformulated  as
\begin{equation}\label{3.2}
	\beta A\mathbf{v} = \alpha B\mathbf{v}, \qquad \mathbf{v} \neq 0
\end{equation}
We have replaced $\lambda$ with $\alpha/\beta$ for convenience so that the generalized eigenvalues will be of the form $(\alpha, \beta)$. If $ \beta = 0$, then the generalized eigenvalues $\Lambda(A, B)$ will be infinite. The formulation using equation(\ref{3.2}) is useful when describing the error bounds, as we shall later see. We shall alternate between (\ref{3.1}) and (\ref{3.2}) when convenient. We also observe that the symmetric-definite generalized eigenvalue problem have real eigenvalues.\\
To compute the eigenvalues and eigenvectors that satisfy equation(\ref{3.1}) with spectral transformation lanczos algorithm, our approach will be in two steps:
\begin{itemize}
	\item[$\bullet$] Transform the generalized problem into a spectral transformed standard eigenvalue problem.
	\item[$\bullet$] Solve the spectral problem with Lanczos algorithm.
\end{itemize}
Let $\sigma \in \mathbb{R}$ be a desired shift such that $A - \sigma B$ is non-singular. The shifted problem takes the form:
\begin{equation}\label{3.3}
	(A - \sigma B)v = (\lambda - \sigma)Bv
\end{equation}
We shall begin by computing decompositions for $A - \sigma B$ and $B$. If $B$ is positive  definite, we can compute a Cholesky decomposition $B = C_bC_b^T$ using SciPy \texttt{cholesky} method which calls LAPACK \textbf{\texttt{xPOTRF}}. However, if $B$ is semi positive definite, this function call fails and we use the more robust pivoted Cholesky factorization \textbf{\texttt{xPSTRF}} by calling the inbuilt LAPACK bindings in SciPy.\\
There are various possible factorization options for $A-\sigma B$. One option is to use the pivoted $LDL^{T}$ factorization used by Michael Stewart(2024) and Thomas Ericsson (1960) where $D$ is a block diagonal matrix with $1 \times 1$ and $2 \times 2$ on the diagonal, and $L$ is a lower triangular matrix. This factorization uses the Bunch-Kaufman pivoting scheme with "rook pivoting" which is stable. Although the standard $LDL^T$ factorization (without "rook pivoting") is available in SciPy linear algebra module, there is no option to use the rook pivoting scheme except if one chooses to write a custom LAPACK binding that makes use of \textbf{\texttt{DSYTRF\_ROOK}}. While this can guarantee some stability for the problem we are trying to solve, it usually involves extra work in processing the $2 \times 2$ blocks to make $D$ diagonal.\\
Another factorization is an eigenvalue decomposition of $A - \sigma B$. If we use a symmetric eigenvalue decomposition $A- \sigma B = UDU^T$, our numerical experiments reveals that this stabilizes the Ritz residuals and generalized form of the residuals together with the advantage that these residuals are insensitive to the conditioning of $A$ and $B$. This can be done using inbuilt eigenvalue solvers in SciPy or any linear algebra library. This is the most promising factorization, however computing eigenvalue decompositions for large problems become computationally expensive and not feasible in reality.

Lastly, we can make use of an $LU$ factorization for $A-\sigma B$. Unlike the previous factorizations, the stability for the Ritz residuals is not as great, as we observe that they depend on the conditioning of $A$ and $B$. However, for the purpose of this thesis, we make use of the $LU$ decomposition since it is computationally less expensive and easy to use and implement.

One major takeaway from our experiments with the various options of factorizing $A-\sigma B$ is that symmetry is clearly important for stability. We plan to give a mathematical justification for this in future work.

Continuing with the algorithm derivation, if we assume $\lambda \neq \infty$ and $\mathbf{v} \neq \mathbf{0}$. Since $B$ is positive definite, Michael Stewart (2024), proved that we can compute a Cholesky factorization $B = C_bC_b^T$, and apply the shift-invert spectral transformation to transform equation(\ref{3.1}) into its spectral form as described in section (\ref{section-2.11}) such that $\theta = 1/(\lambda - \sigma)$ is an eigenvalue of the problem :
\begin{equation}\label{3.4}
	C_b^T (A-\sigma B)^{-1} C_b \mathbf{u} = \theta \mathbf{u}, \qquad \mathbf{u} \neq \mathbf{0}
\end{equation}
where  $\mathbf{u} = C_b^T \mathbf{v} \neq \mathbf{0}.$\\
Conversely, assume that $\mathbf{u} \neq \mathbf{0}$ is an eigenvector of (\ref{3.4}) and $\theta$ its corresponding eigenvalue, then the vector $v = (A-\sigma B)^{-1}C_b \mathbf{u} \neq \mathbf{0}$ is an eigenvector for (\ref{3.2}), with eigenvalue $(1+\sigma \theta, \theta)$, provided $C_b\mathbf{u} \neq \mathbf{0}$.\\[10pt]
Equation (\ref{3.4}) gives us the spectral transformed version of the original generalized problem. Since the problem is now in a standard form, we can then apply the Lanczos algorithm to compute the desired eigenvalues within the neighborhood of $\sigma$, together with their corresponding eigenvectors. It should be noted that forming the spectral matrix in (\ref{3.4}) is not desirable as it will make the Lanczos algorithm unstable. Forming the matrix directly also has the disadvantage that the matrix might no longer be symmetric which could prevent the Lanczos algorithm from converging. The right thing to do is to use the $LU$ for $A-\sigma B$ as explained earlier. This will be explored in the next section.
\section{Lanczos decomposition}
In this section, we revisit the Lanczos algorithm, and discuss how we apply it to the spectral transformed problem. As discussed in section \ref{section2.10}, the Lanczos algorithm approximates the eigenvalues of the original problem by projecting it onto a Krylov subspace spanned by successive powers of the system matrix applied to an initial vector. The eigenvalues approximation arises from the tridiagonal matrix obtained through the Lanczos process, which captures the essential spectral characteristics of the original matrix.\\
Given $A \in \mathbb{R}^{m \times m}$, with $A=A^T$, the pesudocode for the lanczos algorithm is given as follows:
\begin{algorithm}
	\caption{Lanczos Algorithm for a Symmetric Matrix}
	\label{alg:lanczos_algorithm}

	\textbf{Require:} \( A = A^T \), number of iterations: \(n\), tolerance: \(tol\)
	\begin{algorithmic}[1]
		\Function{lanczos}{$A, n, tol$}
		\State Choose an arbitrary vector $b$ and set an initial vector $q_1 = b/ \|b\|_2$ 
		\State Set $\beta_0 = 0$ and $q_0 = 0$
			\For{$j = 1, 2, \dots, n$}
		\State $v = A q_j$
		\State $\alpha_j = q_j^T v $
		\State $v = v - \beta_{j-1}q_{j-1} - \alpha_j q_j$
		\State \textbf{Full reorthogonalization:} $v = v - \sum_{i \leq j} (q_i^T v) q_i$
		\State $\beta_{j} = \|v\|_2$
		\If{$\beta_{j} < tol $}
		\State \textbf{restart} or \textbf{exit}
		\EndIf
		\State $q_{j+1} := v / \beta_{j}$
		\EndFor
		\EndFunction
	\end{algorithmic}
\end{algorithm}\\
After the completion of algorithm \ref{alg:lanczos_algorithm}, the $\alpha$'s and $\beta$'s are used to construct the tridiagonal matrix $T_n \in \mathbb{R}^{n \times n}$ and the vectors $q_j$'s are stacked together to form an orthogonal matrix $Q_n \in \mathbb{R}^{m \times n}$ given by:
\[T_n = \begin{pmatrix}
			\alpha_1 & \beta_1 & & & \\\beta_1 & \alpha_2 & \beta_2 & & \\ & \beta_2 & \alpha_3 & \beta_3 & \\ & & \ddots & \ddots & \vdots \\ & & & \beta_{n-1} & \alpha_n
		\end{pmatrix}\] 
	\[
	Q_n = 
	\begin{bmatrix}
		 & \big| &  & \big| &  & \big| &  \\
		 & \big| &  & \big| &  & \big| &  \\
		 q_1 & \big| & q_2 & \big| & \cdots & \big| & q_n \\
		 & \big| &  & \big| &  & \big| &  \\
		 & \big| &  & \big| &  & \big| &  \\
	\end{bmatrix}.
	\]
The decomposition is given by
\begin{equation}
	AQ_n = Q_nT_n + \beta_{n}q_{n+1}e_n^T
\end{equation}
In theory, the vectors $q_j$'s should be orthonormal, but due to floating-point errors, there will be loss of orthogonalization, hence the need for line 8 in the Algorithm \ref{alg:lanczos_algorithm}.\\
Let $\theta_i, i = 1,2, \ldots n$(which can be computed by standard functions in using any eigenvalue solver) be the eigenvalues of $T_n$, and $\{y_i\}_{i = 1 : n}$ be the associated eigenvectors. The $\{\theta_i\}$ are called the \textit{Ritz values} and the vectors $\{Q_ny_i\}_{i = 1 : n}$ are called the \textit{Ritz vectors}. Hence, the eigenvalues of $A$ are on both ends of the are well approximated by the Ritz values, with the Ritz vectors as their approximate corresponding eigenvectors of $A$.\par
Since the generalized eigenvalue problem we started with has been reduced to a standard one as shown in equation (\ref{3.3}), Algorithm (\ref{alg:lanczos_algorithm}) can be applied to equation (\ref{3.3}) with some slight modifications. We shall now give the spectral form of Algorithm (\ref{alg:lanczos_algorithm}):\\
\begin{algorithm}
	\caption{Spectral Lanczos Algorithm for (\ref{3.4}) }
	\label{alg:spectral_lanczos_algorithm}
	
	\textbf{Require:} \( A = A^T \), \( B = B^T \), with \(B\) being positive definite or semidefinite\\
	\textbf{Require:} number of iterations: \(n\), size of matrix $A$ or $B$: $m$, tolerance: \(tol\)\\
	\textbf{Require:} \(\sigma \in \mathbb{R}\): shift not close to a generalized eigenvalue
	\begin{algorithmic}[1]
		\Function{\textsc{Spectral\_Lanczos}}{$A, B, m, n, \sigma, tol$}
		\State Choose an arbitrary vector $b$ and set an initial vector $q_1 = b/ \|b\|_2$
		\State Set $\beta_0 = 0$ and $q_0 = 0$
		\State Set $Q = zeros(m, n+1)$
		\State Precompute the $LU$ factorization of $A - \sigma B$: $LU = (A - \sigma B)$
		\State Factor: $B = CC^T$
		\For{$j = 1, 2, \dots, n$}
		\State $Q[:, j] = q_j$
		\State $u = Cq_j$
		\State Solve: $(LU)v = u$ for $v$
		\State $v = C^T v$
		\If{$j < n $}
		\State $\alpha_j = q_j^T v $
		\State $v = v - \beta_{j-1}q_{j-1} - \alpha_j q_j$
		\State \textbf{Full reorthogonalization:} $v = v - \sum_{i \leq j} (q_i^T v) q_i$
		\State $\beta_{j} = \|v\|_2$
		\If{$\beta_{j} < tol $}
		\State \textbf{restart} or \textbf{exit}
		\EndIf
		\State $q_{j+1} := v / \beta_{j}$
		\EndIf
		\EndFor
		\State $Q = Q[:, :n]$
		\State $q = Q[:, n]$
		\State \Return $(Q, T, q)$
		\EndFunction
	\end{algorithmic}
\end{algorithm}\\
After applying the lanczos procedure to the spectral transformed problem (\ref{3.4}), we then compute the converged Ritz pairs using a certain tolerance. The converged Ritz pairs are mapped to the generalized eigenvalues and eigenvectors where we can observe the behaviour of these residuals with respect to conditioning.
\section{Experimental Setup}
To evaluate the performance and robustness of the spectral transformation lanczos algorithm, we setup a problem with predetermined eigenvalues, use the algorithm to compute the eigenvalues, and show that the residuals follow closely with the bounds predicted by direct methods. While there are other options of using matrices from open source repositories like Matrix Market, we choose to use this approach so that we can control the size, condition number and other properties of the matrix so as to observe the effect of this properties on the algorithm.

Starting with a diagonal matrix $D \in \mathbb{R}^{m \times m}$ with known eigenvalues, we generate a random matrix $P$ of size $m \times m $ with standard normal distribution. Since the $QR$ factorization is guaranteed to exist for any matrix, we take the $QR$ factorization of $P$ to obtain an orthogonal matrix $Q$, which is used to create a matrix $C$ using orthogonal transformation. Hence $C = QDQ^T$ is unitarily similar to $D$.

Next, we initialize a random lower triangular matrix $L_0 \in \mathbb{R}^{m \times m}$ with a normal distribution. A symmetric positive definite $B \in \mathbb{R}$ is formed by
\begin{equation}
	B = L_0 L_0^T + \delta I_m, \qquad \delta > 0
\end{equation}
where $I_m$ is an identity matrix of order $m$. Clearly, $B$ is symmetric. The matrix $L_0L_0^T$ is positive semi-definite since for any non-zero vector $\mathbf{x}$
\begin{equation}
	\mathbf{x}^T(L_0L_0^T)\mathbf{x} = (L_0^T\mathbf{x})^T(L_0^T\mathbf{x}) = \| L_0^T\mathbf{x} \|^2 \geq \mathbf{0}
\end{equation}
However, $L_0L_0^T$ may not be strictly positive definite if $L_0$ is singular. The term $\delta I_m$ ensures strict positve definiteness by adding $\delta$ to its diagonals, thereby shifting all eigenvalues by $\delta$. If $\delta > 0$, then all eigenvalues of $B$ will be strictly positive, ensuring $B$ is positive definite. This guarantees that we can compute the Cholesky factorization of $B$ without any numerical issues.

Another important thing to note is that, $\delta$ can be used to control the conditioning of $B$. We recall from section (\ref{section1.2.6}), that the condition number of $B$ when $B$ is symmetric, is defined as:
\begin{equation}
	\kappa(B) = \frac{\lambda_{\max}(B)}{\lambda_{\min}(B)}
\end{equation}
where $\lambda_{\max}(B)$ and $\lambda_{\min}(B)$ are the largest and smallest eigenvalues of B, respectively.
In general, $B$ is usually ill-conditioned with a very large condition number so that if $\delta$ is large, the process of adding $\delta I_m$ can regularize the condition number of $B$, making $B$ well-conditioned, since that will equate to increasing $\lambda_{\min}(B)$. If delta is small, $B$ can still be ill-conditioned but not in an astronomical way. Hence, $\delta$ is a hyperparameter we can use to control the condition of $B$. In this experiment, we choose $\delta = 10^{-2}$, which gives a condition number of $\kappa(B) = 5.39 \times 10^5$.\\
Since $B$ is symmetric and positive definite, we can compute it's Cholesky factorization $B = LL^T$ and construct $A$ using a congruence transformation
\begin{equation}\label{3.9}
	A = LCL^T
\end{equation}
So that the generalized eigenvalues $\Lambda(A, B)$ is equal to the eigenvalues of the diagonal matrix $D$. This can be summarized by the following lemma:
\begin{lemma}
	Let $A-\lambda B$ be a pencil, where $A$ and $B$ are symmetric, and $B$ is strictly positive definite. Let $D$ be a diagonal matrix and $C$ be unitarily similar to $D$. Assuming (\ref{3.9}) holds, then the generalized eigenvalues $\Lambda(A, B)$ is similar to $D$
\end{lemma}

\begin{proof}
	Given the generalized problem
	\begin{equation}
		A \mathbf{v} = \lambda B \mathbf{v}, \qquad \mathbf{v} \neq \mathbf{0}
	\end{equation}
	Since $B$ is positive definite, then clearly, it is invertible and the generalized eigenvalues $\Lambda(A, B)$ will be the eigenvalues of $B^{-1}A$.\\
	Now
	\begin{align*}
		B^{-1}A & = (LL^T)^{-1}(LCL^T)\\
		& = L^{-T}L^{-1}LQDQ^{T}L^T\\
		& = (L^{-T}Q)D(Q^{-1}L^{T}) \\
		& = (L^{-T}Q)D(L^{-T}Q)^{-1}
	\end{align*}
	Therefore $B^{-1}A$ is similar to $D$ and hence $\Lambda(A,B)$ is similar to $D$.
\end{proof}
The pseudocode for generating $A$ and $B$ is given as follows:
\begin{algorithm}
	\caption{Setting up a GEP}
	\label{alg:problem setup}
	
	\textbf{Require:} \( D \): diagonal matrix with known eigenvalues, \(\delta\): regularization hyperparameter
	\begin{algorithmic}[1]
		\Function{\textsc{Generate\_Matrix}}{$D, \delta$}
		\State Set $m$ = \texttt{size}($D$)
		\State $Q$, \_\_ = \texttt{qr}(random.randn($m$, $m$))
		\State $C = QDQ^T$
		\State $L_{0}$ = \texttt{tril}(\texttt{random.randn}($m$, $m$))
		\State $B = (L_0 L_0^T) + \delta \cdot eye (m)$
		\State $L$ = \texttt{cholesky}($B$)
		\State $A$ = $LCL^T$
		\State \Return ($A$, $B$)
		\EndFunction
	\end{algorithmic}
\end{algorithm}\\
With the problem setup completed, and the algorithm described, in the next chapter, we shall discuss the results obtained in these experiments.






















